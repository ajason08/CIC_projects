{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myarg= '--data_dir dataset/tacred --vocab_dir dataset/vocab --id 00 --info \"Position-aware attention model\"'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_dir DATA_DIR]\n",
      "                             [--vocab_dir VOCAB_DIR] [--emb_dim EMB_DIM]\n",
      "                             [--ner_dim NER_DIM] [--pos_dim POS_DIM]\n",
      "                             [--hidden_dim HIDDEN_DIM]\n",
      "                             [--num_layers NUM_LAYERS] [--dropout DROPOUT]\n",
      "                             [--word_dropout WORD_DROPOUT] [--topn TOPN]\n",
      "                             [--lower] [--no-lower] [--attn] [--no-attn]\n",
      "                             [--attn_dim ATTN_DIM] [--pe_dim PE_DIM] [--lr LR]\n",
      "                             [--lr_decay LR_DECAY] [--optim OPTIM]\n",
      "                             [--num_epoch NUM_EPOCH] [--batch_size BATCH_SIZE]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--log_step LOG_STEP] [--log LOG]\n",
      "                             [--save_epoch SAVE_EPOCH] [--save_dir SAVE_DIR]\n",
      "                             [--id ID] [--info INFO] [--seed SEED]\n",
      "                             [--cuda CUDA] [--cpu]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --data_dir dataset/tacred --vocab_dir dataset/vocab --id 00 --info \"Position-aware attention model\"\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train a model on TACRED.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import scorer, constant, helper\n",
    "from utils.vocab import Vocab\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "#\"\"\"\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--vocab_dir', type=str, default='dataset/vocab')\n",
    "parser.add_argument('--emb_dim', type=int, default=300, help='Word embedding dimension.')\n",
    "parser.add_argument('--ner_dim', type=int, default=30, help='NER embedding dimension.')\n",
    "parser.add_argument('--pos_dim', type=int, default=30, help='POS embedding dimension.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=200, help='RNN hidden state size.')\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='Num of RNN layers.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help='Input and RNN dropout rate.')\n",
    "parser.add_argument('--word_dropout', type=float, default=0.04, help='The rate at which randomly set a word to UNK.')\n",
    "parser.add_argument('--topn', type=int, default=1e10, help='Only finetune top N embeddings.')\n",
    "parser.add_argument('--lower', dest='lower', action='store_true', help='Lowercase all words.')\n",
    "parser.add_argument('--no-lower', dest='lower', action='store_false')\n",
    "parser.set_defaults(lower=False)\n",
    "\n",
    "parser.add_argument('--attn', dest='attn', action='store_true', help='Use attention layer.')\n",
    "parser.add_argument('--no-attn', dest='attn', action='store_false')\n",
    "parser.set_defaults(attn=True)\n",
    "parser.add_argument('--attn_dim', type=int, default=200, help='Attention size.')\n",
    "parser.add_argument('--pe_dim', type=int, default=30, help='Position encoding dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=1.0, help='Applies to SGD and Adagrad.')\n",
    "parser.add_argument('--lr_decay', type=float, default=0.9)\n",
    "parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')\n",
    "parser.add_argument('--num_epoch', type=int, default=30)\n",
    "parser.add_argument('--batch_size', type=int, default=50)\n",
    "parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')\n",
    "parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')\n",
    "parser.add_argument('--log', type=str, default='logs.txt', help='Write training log to file.')\n",
    "parser.add_argument('--save_epoch', type=int, default=5, help='Save model checkpoints every k epochs.')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models', help='Root dir for saving models.')\n",
    "parser.add_argument('--id', type=str, default='00', help='Model ID under which to save models.')\n",
    "#parser.add_argument('--info', type=str, default='', help='Optional info for the experiment.')\n",
    "parser.add_argument('--info', type=str, default='', help='Position-aware attention model')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')\n",
    "#args = parser.parse_args()\n",
    "#\"\"\"\n",
    "args = parser.parse_args([str(myarg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 421 loaded from file\n",
      "Loading data from dataset/tacred with batch size 50...\n",
      "1 batches created for dataset/tacred/train.json\n",
      "1 batches created for dataset/tacred/dev.json\n",
      "Config saved to file ./saved_models/00/config.json\n",
      "Overwriting old vocab file at ./saved_models/00/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 200\n",
      "\tnum_layers : 2\n",
      "\tdropout : 0.5\n",
      "\tword_dropout : 0.04\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tattn : True\n",
      "\tattn_dim : 200\n",
      "\tpe_dim : 30\n",
      "\tlr : 1.0\n",
      "\tlr_decay : 0.9\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 30\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 5.0\n",
      "\tlog_step : 20\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 5\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 00\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : False\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 421\n",
      "\tmodel_save_dir : ./saved_models/00\n",
      "\n",
      "\n",
      "Finetune all embeddings.\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 1: train_loss = 9.361523, dev_loss = 6.023990, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 2: train_loss = 4.473186, dev_loss = 6.735688, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_2.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 3: train_loss = 1.644053, dev_loss = 5.787633, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_3.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 4: train_loss = 1.274011, dev_loss = 5.905735, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_4.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 5: train_loss = 1.127131, dev_loss = 6.205078, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_5.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 6: train_loss = 0.977746, dev_loss = 6.521367, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_6.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 7: train_loss = 0.910068, dev_loss = 6.928499, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_7.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 8: train_loss = 0.855917, dev_loss = 7.153214, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_8.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 9: train_loss = 0.838122, dev_loss = 7.458441, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_9.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 10: train_loss = 0.826347, dev_loss = 7.530245, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_10.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 11: train_loss = 0.840577, dev_loss = 7.990891, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_11.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 12: train_loss = 0.863580, dev_loss = 7.698416, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_12.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 13: train_loss = 0.923546, dev_loss = 8.985174, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_13.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 14: train_loss = 1.234624, dev_loss = 7.670812, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_14.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 15: train_loss = 0.813213, dev_loss = 7.823931, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_15.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 16: train_loss = 0.829316, dev_loss = 7.820196, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_16.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 17: train_loss = 0.811983, dev_loss = 8.026035, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_17.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 18: train_loss = 0.809216, dev_loss = 7.989284, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_18.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 19: train_loss = 0.801085, dev_loss = 8.090972, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_19.pt\n",
      "\n",
      "2019-02-22 15:44:08.635071: step 20/30 (epoch 20/30), loss = 0.305322 (0.327 sec/batch), lr: 0.430467\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 20: train_loss = 0.763305, dev_loss = 8.127823, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_20.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 21: train_loss = 0.786008, dev_loss = 8.178142, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_21.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 22: train_loss = 0.778397, dev_loss = 8.220073, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_22.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 23: train_loss = 0.788809, dev_loss = 8.238322, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_23.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 24: train_loss = 0.780575, dev_loss = 8.257478, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_24.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 25: train_loss = 0.765652, dev_loss = 8.299557, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_25.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 26: train_loss = 0.778099, dev_loss = 8.312985, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_26.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 27: train_loss = 0.787540, dev_loss = 8.326529, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_27.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 28: train_loss = 0.774203, dev_loss = 8.338831, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_28.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 29: train_loss = 0.778248, dev_loss = 8.348495, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_29.pt\n",
      "\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 30: train_loss = 0.753896, dev_loss = 8.368303, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/00/checkpoint_epoch_30.pt\n",
      "\n",
      "Training ended with 30 epochs.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(1234)\n",
    "if args.cpu:\n",
    "    args.cuda = False\n",
    "elif args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# make opt\n",
    "opt = vars(args)\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "\n",
    "# load vocab\n",
    "vocab_file = opt['vocab_dir'] + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)\n",
    "opt['vocab_size'] = vocab.size\n",
    "emb_file = opt['vocab_dir'] + '/embedding.npy'\n",
    "emb_matrix = np.load(emb_file)\n",
    "assert emb_matrix.shape[0] == vocab.size\n",
    "assert emb_matrix.shape[1] == opt['emb_dim']\n",
    "\n",
    "# load data\n",
    "print(\"Loading data from {} with batch size {}...\".format(opt['data_dir'], opt['batch_size']))\n",
    "train_batch = DataLoader(opt['data_dir'] + '/train.json', opt['batch_size'], opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(opt['data_dir'] + '/dev.json', opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "model_id = opt['id'] if len(opt['id']) > 1 else '0' + opt['id']\n",
    "model_save_dir = opt['save_dir'] + '/' + model_id\n",
    "opt['model_save_dir'] = model_save_dir\n",
    "helper.ensure_dir(model_save_dir, verbose=True)\n",
    "\n",
    "# save config\n",
    "helper.save_config(opt, model_save_dir + '/config.json', verbose=True)\n",
    "vocab.save(model_save_dir + '/vocab.pkl')\n",
    "file_logger = helper.FileLogger(model_save_dir + '/' + opt['log'], header=\"# epoch\\ttrain_loss\\tdev_loss\\tdev_f1\")\n",
    "\n",
    "# print model info\n",
    "helper.print_config(opt)\n",
    "\n",
    "# model\n",
    "model = RelationModel(opt, emb_matrix=emb_matrix)\n",
    "\n",
    "id2label = dict([(v,k) for k,v in constant.LABEL_TO_ID.items()])\n",
    "dev_f1_history = []\n",
    "current_lr = opt['lr']\n",
    "\n",
    "global_step = 0\n",
    "global_start_time = time.time()\n",
    "format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'\n",
    "max_steps = len(train_batch) * opt['num_epoch']\n",
    "\n",
    "# start training\n",
    "for epoch in range(1, opt['num_epoch']+1):\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_batch):\n",
    "        start_time = time.time()\n",
    "        global_step += 1\n",
    "        loss = model.update(batch)\n",
    "        train_loss += loss\n",
    "        if global_step % opt['log_step'] == 0:\n",
    "            duration = time.time() - start_time\n",
    "            print(format_str.format(datetime.now(), global_step, max_steps, epoch,\\\n",
    "                    opt['num_epoch'], loss, duration, current_lr))\n",
    "\n",
    "    # eval on dev\n",
    "    print(\"Evaluating on dev set...\")\n",
    "    predictions = []\n",
    "    dev_loss = 0\n",
    "    for i, batch in enumerate(dev_batch):\n",
    "        preds, _, loss = model.predict(batch)\n",
    "        predictions += preds\n",
    "        dev_loss += loss\n",
    "    predictions = [id2label[p] for p in predictions]\n",
    "    dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions)\n",
    "    \n",
    "    train_loss = train_loss / train_batch.num_examples * opt['batch_size'] # avg loss per batch\n",
    "    dev_loss = dev_loss / dev_batch.num_examples * opt['batch_size']\n",
    "    print(\"epoch {}: train_loss = {:.6f}, dev_loss = {:.6f}, dev_f1 = {:.4f}\".format(epoch,\\\n",
    "            train_loss, dev_loss, dev_f1))\n",
    "    file_logger.log(\"{}\\t{:.6f}\\t{:.6f}\\t{:.4f}\".format(epoch, train_loss, dev_loss, dev_f1))\n",
    "\n",
    "    # save\n",
    "    model_file = model_save_dir + '/checkpoint_epoch_{}.pt'.format(epoch)\n",
    "    model.save(model_file, epoch)\n",
    "    if epoch == 1 or dev_f1 > max(dev_f1_history):\n",
    "        copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "        print(\"new best model saved.\")\n",
    "    if epoch % opt['save_epoch'] != 0:\n",
    "        os.remove(model_file)\n",
    "    \n",
    "    # lr schedule\n",
    "    if len(dev_f1_history) > 10 and dev_f1 <= dev_f1_history[-1] and \\\n",
    "            opt['optim'] in ['sgd', 'adagrad']:\n",
    "        current_lr *= opt['lr_decay']\n",
    "        model.update_lr(current_lr)\n",
    "\n",
    "    dev_f1_history += [dev_f1]\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Training ended with {} epochs.\".format(epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from saved_models/00/best_model.pt\n",
      "Finetune all embeddings.\n",
      "Vocab size 421 loaded from file\n",
      "Loading data from dataset/tacred/test.json with batch size 50...\n",
      "1 batches created for dataset/tacred/test.json\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 200\n",
      "\tnum_layers : 2\n",
      "\tdropout : 0.5\n",
      "\tword_dropout : 0.04\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tattn : True\n",
      "\tattn_dim : 200\n",
      "\tpe_dim : 30\n",
      "\tlr : 1.0\n",
      "\tlr_decay : 0.9\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 30\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 5.0\n",
      "\tlog_step : 20\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 5\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 00\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : False\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 421\n",
      "\tmodel_save_dir : ./saved_models/00\n",
      "\n",
      "\n",
      "Per-relation statistics:\n",
      "org:top_members/employees  P: 100.00%  R:   0.00%  F1:   0.00%  #: 1\n",
      "per:age                    P: 100.00%  R:   0.00%  F1:   0.00%  #: 1\n",
      "per:origin                 P: 100.00%  R:   0.00%  F1:   0.00%  #: 1\n",
      "per:title                  P: 100.00%  R:   0.00%  F1:   0.00%  #: 1\n",
      "\n",
      "Final Score:\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "Evaluation ended.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run evaluation with saved models.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import torch_utils, scorer, constant, helper\n",
    "from utils.vocab import Vocab\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('model_dir', type=str, help='Directory of the model.')\n",
    "parser.add_argument('--model', type=str, default='best_model.pt', help='Name of the model file.')\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--dataset', type=str, default='test', help=\"Evaluate on dev or test.\")\n",
    "parser.add_argument('--out', type=str, default='', help=\"Save model predictions to this dir.\")\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true')\n",
    "args = parser.parse_args([\"saved_models/00\"])\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(1234)\n",
    "if args.cpu:\n",
    "    args.cuda = False\n",
    "elif args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# load opt\n",
    "model_file = args.model_dir + '/' + args.model\n",
    "print(\"Loading model from {}\".format(model_file))\n",
    "opt = torch_utils.load_config(model_file)\n",
    "model = RelationModel(opt)\n",
    "model.load(model_file)\n",
    "\n",
    "# load vocab\n",
    "vocab_file = args.model_dir + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)\n",
    "assert opt['vocab_size'] == vocab.size, \"Vocab size must match that in the saved model.\"\n",
    "\n",
    "# load data\n",
    "data_file = opt['data_dir'] + '/{}.json'.format(args.dataset)\n",
    "print(\"Loading data from {} with batch size {}...\".format(data_file, opt['batch_size']))\n",
    "batch = DataLoader(data_file, opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "helper.print_config(opt)\n",
    "id2label = dict([(v,k) for k,v in constant.LABEL_TO_ID.items()])\n",
    "\n",
    "predictions = []\n",
    "all_probs = []\n",
    "for i, b in enumerate(batch):\n",
    "    preds, probs, _ = model.predict(b)\n",
    "    predictions += preds\n",
    "    all_probs += probs\n",
    "predictions = [id2label[p] for p in predictions]\n",
    "p, r, f1 = scorer.score(batch.gold(), predictions, verbose=True)\n",
    "\n",
    "# save probability scores\n",
    "if len(args.out) > 0:\n",
    "    helper.ensure_dir(os.path.dirname(args.out))\n",
    "    with open(args.out, 'wb') as outfile:\n",
    "        pickle.dump(all_probs, outfile)\n",
    "    print(\"Prediction scores saved to {}.\".format(args.out))\n",
    "\n",
    "print(\"Evaluation ended.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no_relation': 0,\n",
       " 'per:title': 1,\n",
       " 'org:top_members/employees': 2,\n",
       " 'per:employee_of': 3,\n",
       " 'org:alternate_names': 4,\n",
       " 'org:country_of_headquarters': 5,\n",
       " 'per:countries_of_residence': 6,\n",
       " 'org:city_of_headquarters': 7,\n",
       " 'per:cities_of_residence': 8,\n",
       " 'per:age': 9,\n",
       " 'per:stateorprovinces_of_residence': 10,\n",
       " 'per:origin': 11,\n",
       " 'org:subsidiaries': 12,\n",
       " 'org:parents': 13,\n",
       " 'per:spouse': 14,\n",
       " 'org:stateorprovince_of_headquarters': 15,\n",
       " 'per:children': 16,\n",
       " 'per:other_family': 17,\n",
       " 'per:alternate_names': 18,\n",
       " 'org:members': 19,\n",
       " 'per:siblings': 20,\n",
       " 'per:schools_attended': 21,\n",
       " 'per:parents': 22,\n",
       " 'per:date_of_death': 23,\n",
       " 'org:member_of': 24,\n",
       " 'org:founded_by': 25,\n",
       " 'org:website': 26,\n",
       " 'per:cause_of_death': 27,\n",
       " 'org:political/religious_affiliation': 28,\n",
       " 'org:founded': 29,\n",
       " 'per:city_of_death': 30,\n",
       " 'org:shareholders': 31,\n",
       " 'org:number_of_employees/members': 32,\n",
       " 'per:date_of_birth': 33,\n",
       " 'per:city_of_birth': 34,\n",
       " 'per:charges': 35,\n",
       " 'per:stateorprovince_of_death': 36,\n",
       " 'per:religion': 37,\n",
       " 'per:stateorprovince_of_birth': 38,\n",
       " 'per:country_of_birth': 39,\n",
       " 'org:dissolved': 40,\n",
       " 'per:country_of_death': 41}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant.LABEL_TO_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6lm",
   "language": "python",
   "name": "lm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
