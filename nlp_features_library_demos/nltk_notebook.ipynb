{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "\n",
    "path_to_jar = 'C:/my_temp/stanford-corenlp-full-2018-02-27/stanford-parser.jar'\n",
    "path_to_models_jar = 'C:/my_temp/stanford-corenlp-full-2018-02-27/stanford-corenlp-3.9.1-models.jar'\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, \n",
    "                                             path_to_models_jar=path_to_models_jar,\n",
    "                                             verbose=True)\n",
    "#entence = \"He can't shot some elephants in my sleep\"\n",
    "sentence = \"maintenance of th1 responses and dendritic cell (dc ) functions are compromised in hiv-1 infected individuals.\"\n",
    "result = dependency_parser.raw_parse(sentence)\n",
    "dep = next(result)\n",
    "#dep2 =next(dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dep.nodes.keys()\n",
    "#print(dep.nodes.items(),\"\\n\")\n",
    "#print (dep.nodes.values())\n",
    "#dep.nodes.get(2)\n",
    "#nodes_dic = dep.nodes\n",
    "#x = dep.nx_graph()\n",
    "#map_labels = dep.nx_labels\n",
    "#inv_map = {v: k for k, v in map_labels.items()}\n",
    "#inv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 root 13\n",
      "1 nmod 4\n",
      "1 cc 5\n",
      "1 conj 11\n",
      "4 case 2\n",
      "4 amod 3\n",
      "11 amod 6\n",
      "11 compound 7\n",
      "11 appos 9\n",
      "13 nsubjpass 1\n",
      "13 auxpass 12\n",
      "13 nmod 17\n"
     ]
    }
   ],
   "source": [
    "# get dependencies\n",
    "nodes_dic = dep.nodes\n",
    "for origin in range(len(nodes_dic)):\n",
    "    deps_origin = nodes_dic[origin]['deps']\n",
    "    for dep_key in deps_origin:\n",
    "        target = deps_origin[dep_key]\n",
    "        if len(target)>0:\n",
    "            print (origin,dep_key,target[0])\n",
    "\n",
    "# another not so controled but easy way    \n",
    "# triples = list(dep.triples())\n",
    "# for triple in triples:\n",
    "#    print (triple[0][0],triple[1],triple[2][0])#, triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function nltk.parse.dependencygraph.DependencyGraph.__init__.<locals>.<lambda>>,\n",
       "            {0: {'address': 0,\n",
       "              'ctag': 'TOP',\n",
       "              'deps': defaultdict(list, {'root': [13]}),\n",
       "              'feats': None,\n",
       "              'head': None,\n",
       "              'lemma': None,\n",
       "              'rel': None,\n",
       "              'tag': 'TOP',\n",
       "              'word': None},\n",
       "             1: {'address': 1,\n",
       "              'ctag': 'NN',\n",
       "              'deps': defaultdict(list,\n",
       "                          {'cc': [5], 'conj': [11], 'nmod': [4]}),\n",
       "              'feats': '_',\n",
       "              'head': 13,\n",
       "              'lemma': '_',\n",
       "              'rel': 'nsubjpass',\n",
       "              'tag': 'NN',\n",
       "              'word': 'maintenance'},\n",
       "             2: {'address': 2,\n",
       "              'ctag': 'IN',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': '_',\n",
       "              'head': 4,\n",
       "              'lemma': '_',\n",
       "              'rel': 'case',\n",
       "              'tag': 'IN',\n",
       "              'word': 'of'},\n",
       "             3: {'address': 3,\n",
       "              'ctag': 'JJ',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': '_',\n",
       "              'head': 4,\n",
       "              'lemma': '_',\n",
       "              'rel': 'amod',\n",
       "              'tag': 'JJ',\n",
       "              'word': 'th1'},\n",
       "             4: {'address': 4,\n",
       "              'ctag': 'NNS',\n",
       "              'deps': defaultdict(list, {'amod': [3], 'case': [2]}),\n",
       "              'feats': '_',\n",
       "              'head': 1,\n",
       "              'lemma': '_',\n",
       "              'rel': 'nmod',\n",
       "              'tag': 'NNS',\n",
       "              'word': 'responses'},\n",
       "             5: {'address': 5,\n",
       "              'ctag': 'CC',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': '_',\n",
       "              'head': 1,\n",
       "              'lemma': '_',\n",
       "              'rel': 'cc',\n",
       "              'tag': 'CC',\n",
       "              'word': 'and'},\n",
       "             6: {'address': 6,\n",
       "              'ctag': 'JJ',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': '_',\n",
       "              'head': 11,\n",
       "              'lemma': '_',\n",
       "              'rel': 'amod',\n",
       "              'tag': 'JJ',\n",
       "              'word': 'dendritic'},\n",
       "             7: {'address': 7,\n",
       "              'ctag': 'NN',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': '_',\n",
       "              'head': 11,\n",
       "              'lemma': '_',\n",
       "              'rel': 'compound',\n",
       "              'tag': 'NN',\n",
       "              'word': 'cell'},\n",
       "             8: {'address': None,\n",
       "              'ctag': None,\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': None,\n",
       "              'head': None,\n",
       "              'lemma': None,\n",
       "              'rel': None,\n",
       "              'tag': None,\n",
       "              'word': None},\n",
       "             9: {'address': 9,\n",
       "              'ctag': 'NN',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': '_',\n",
       "              'head': 11,\n",
       "              'lemma': '_',\n",
       "              'rel': 'appos',\n",
       "              'tag': 'NN',\n",
       "              'word': 'dc'},\n",
       "             10: {'address': None,\n",
       "              'ctag': None,\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': None,\n",
       "              'head': None,\n",
       "              'lemma': None,\n",
       "              'rel': None,\n",
       "              'tag': None,\n",
       "              'word': None},\n",
       "             11: {'address': 11,\n",
       "              'ctag': 'NNS',\n",
       "              'deps': defaultdict(list,\n",
       "                          {'amod': [6], 'appos': [9], 'compound': [7]}),\n",
       "              'feats': '_',\n",
       "              'head': 1,\n",
       "              'lemma': '_',\n",
       "              'rel': 'conj',\n",
       "              'tag': 'NNS',\n",
       "              'word': 'functions'},\n",
       "             12: {'address': 12,\n",
       "              'ctag': 'VBP',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': '_',\n",
       "              'head': 13,\n",
       "              'lemma': '_',\n",
       "              'rel': 'auxpass',\n",
       "              'tag': 'VBP',\n",
       "              'word': 'are'},\n",
       "             13: {'address': 13,\n",
       "              'ctag': 'VBN',\n",
       "              'deps': defaultdict(list,\n",
       "                          {'auxpass': [12], 'nmod': [17], 'nsubjpass': [1]}),\n",
       "              'feats': '_',\n",
       "              'head': 0,\n",
       "              'lemma': '_',\n",
       "              'rel': 'root',\n",
       "              'tag': 'VBN',\n",
       "              'word': 'compromised'},\n",
       "             14: {'address': 14,\n",
       "              'ctag': 'IN',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': '_',\n",
       "              'head': 17,\n",
       "              'lemma': '_',\n",
       "              'rel': 'case',\n",
       "              'tag': 'IN',\n",
       "              'word': 'in'},\n",
       "             15: {'address': 15,\n",
       "              'ctag': 'JJ',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': '_',\n",
       "              'head': 17,\n",
       "              'lemma': '_',\n",
       "              'rel': 'amod',\n",
       "              'tag': 'JJ',\n",
       "              'word': 'hiv-1'},\n",
       "             16: {'address': 16,\n",
       "              'ctag': 'JJ',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': '_',\n",
       "              'head': 17,\n",
       "              'lemma': '_',\n",
       "              'rel': 'amod',\n",
       "              'tag': 'JJ',\n",
       "              'word': 'infected'},\n",
       "             17: {'address': 17,\n",
       "              'ctag': 'NNS',\n",
       "              'deps': defaultdict(list, {'amod': [15, 16], 'case': [14]}),\n",
       "              'feats': '_',\n",
       "              'head': 13,\n",
       "              'lemma': '_',\n",
       "              'rel': 'nmod',\n",
       "              'tag': 'NNS',\n",
       "              'word': 'individuals'}})"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 None TOP None\n",
      "1 maintenance NN maintenance\n",
      "2 of IN of\n",
      "3 th1 JJ th1\n",
      "4 responses NNS response\n",
      "5 and CC and\n",
      "6 dendritic JJ dendritic\n",
      "7 cell NN cell\n",
      "8 None None None\n",
      "9 dc NN dc\n",
      "10 None None None\n",
      "11 functions NNS function\n",
      "12 are VBP be\n",
      "13 compromised VBN compromise\n",
      "14 in IN in\n",
      "15 hiv-1 JJ hiv-1\n",
      "16 infected JJ infected\n",
      "17 individuals NNS individual\n"
     ]
    }
   ],
   "source": [
    "# get morphology analysis\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for node in range(len(nodes_dic)):\n",
    "    w_id = node\n",
    "    form = nodes_dic[node]['word']\n",
    "    pos = nodes_dic[node]['tag']\n",
    "    try:\n",
    "        wn_pos = pos[0].lower()\n",
    "        lemma = wnl.lemmatize(form,wn_pos) if wn_pos in ['a','n','v'] else form\n",
    "    except:\n",
    "        lemma = form\n",
    "    print (w_id,form,pos,lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumption NN resumption\n",
      "of IN of\n",
      "the DT the\n",
      "session NN session\n",
      "I PRP i\n",
      "declare VBP declare\n",
      "resumed VBD resume\n",
      "the DT the\n",
      "session NN session\n",
      "of IN of\n",
      "the DT the\n",
      "European NNP european\n",
      "Parliament NNP parliament\n",
      "adjourned VBD adjourn\n",
      "on IN on\n",
      "Friday NNP friday\n",
      "17 CD 17\n",
      "December NNP december\n",
      "1999 CD 1999\n",
      ", , ,\n",
      "and CC and\n",
      "I PRP i\n",
      "would MD would\n",
      "like VB like\n",
      "once RB once\n",
      "again RB again\n",
      "to TO to\n",
      "wish VB wish\n",
      "you PRP you\n",
      "a DT a\n",
      "happy JJ happy\n",
      "new JJ new\n",
      "year NN year\n",
      "in IN in\n",
      "the DT the\n",
      "hope NN hope\n",
      "that IN that\n",
      "you PRP you\n",
      "enjoyed VBP enjoy\n",
      "a DT a\n",
      "pleasant JJ pleasant\n",
      "festive JJ festive\n",
      "period NN period\n",
      ". . .\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_lemma_pos(txt):    \n",
    "    forms = []\n",
    "    poss = []\n",
    "    lemmas = []\n",
    "    wnl = WordNetLemmatizer() \n",
    "    for form, pos in pos_tag(word_tokenize(txt)):\n",
    "        try:\n",
    "            wn_pos = pos[0].lower()\n",
    "            lemma = wnl.lemmatize(form,wn_pos) if wn_pos in ['a','n','v'] else form\n",
    "        except:\n",
    "            lemma = form\n",
    "        lemma = lemma.lower()\n",
    "        forms.append(form)\n",
    "        poss.append(pos)\n",
    "        lemmas.append(lemma)     \n",
    "    return forms, poss, lemmas\n",
    "\n",
    "\n",
    "txt = \"\"\"Resumption of the session I declare resumed the session of the European \n",
    "Parliament adjourned on Friday 17 December 1999 , and I would like once again to\n",
    "wish you a happy new year in the hope that you enjoyed a pleasant festive period .\"\"\"\n",
    "\n",
    "forms1, poss1, lemmas1 = get_lemma_pos(txt)\n",
    "for i, _ in enumerate(forms1):\n",
    "    print (forms1[i], poss1[i], lemmas1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Resumption',\n",
       " 'of',\n",
       " 'the',\n",
       " 'session',\n",
       " 'I',\n",
       " 'declare',\n",
       " 'resume',\n",
       " 'the',\n",
       " 'session',\n",
       " 'of',\n",
       " 'the',\n",
       " 'European',\n",
       " 'Parliament',\n",
       " 'adjourn',\n",
       " 'on',\n",
       " 'Friday',\n",
       " '17',\n",
       " 'December',\n",
       " '1999',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'would',\n",
       " 'like',\n",
       " 'once',\n",
       " 'again',\n",
       " 'to',\n",
       " 'wish',\n",
       " 'you',\n",
       " 'a',\n",
       " 'happy',\n",
       " 'new',\n",
       " 'year',\n",
       " 'in',\n",
       " 'the',\n",
       " 'hope',\n",
       " 'that',\n",
       " 'you',\n",
       " 'enjoy',\n",
       " 'a',\n",
       " 'pleasant',\n",
       " 'festive',\n",
       " 'period',\n",
       " '.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "txt = \"\"\"Resumption of the session I declare resumed the session of the European \n",
    "Parliament adjourned on Friday 17 December 1999 , and I would like once again to\n",
    "wish you a happy new year in the hope that you enjoyed a pleasant festive period .\"\"\"\n",
    "[wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(txt))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hoy', 'me', 'fui', 'al', 'parque', 'porque', 'estaba', 'muy', 'aburrido', 'en', 'la', 'casa', 'de', 'maria']\n",
      "Hoy fui\n",
      "Hoy me\n",
      "me al\n",
      "me fui\n",
      "fui parque\n",
      "fui al\n",
      "al porque\n",
      "al parque\n",
      "parque estaba\n",
      "parque porque\n",
      "porque muy\n",
      "porque estaba\n",
      "estaba aburrido\n",
      "estaba muy\n",
      "muy en\n",
      "muy aburrido\n",
      "aburrido la\n",
      "aburrido en\n",
      "en casa\n",
      "en la\n",
      "la de\n",
      "la casa\n",
      "casa maria\n",
      "casa de\n",
      "de maria\n"
     ]
    }
   ],
   "source": [
    "# co/ocurrence with windows size = 2\n",
    "txt = \"Hoy me fui al parque porque estaba muy aburrido en la casa de maria\"\n",
    "        list_words = word_tokenize(txt)\n",
    "        mylen = len(list_words)\n",
    "        print(list_words)\n",
    "        for i, word in enumerate(list_words):\n",
    "            if i< mylen-2:    \n",
    "                print (word, list_words[i+2])\n",
    "                print (word, list_words[i+1])\n",
    "            elif i< mylen-1:        \n",
    "                print (word, list_words[i+1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
